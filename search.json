[{"title":"DataNode坏盘识别恢复","url":"/2019/05/31/2019-7-25/","content":"<Excerpt in index | 首页摘要>\n解决在实际场景中遇到的盘的修复识别问题\n<!-- more -->\n\n\n## 一、相关背景\nDN上有很多个磁盘，当有某个磁盘坏掉时候，会被识别并上报。但是每次换完磁盘后需要重启DN才能使的新换的盘被NN识别，这会导致换盘的过程异常的麻烦\n## 二、问题分析\n### 1.DN是如何判断某个盘是损坏的？\n1. FsVolumeList通过volumeFailureInfos来记录所有的失败卷的信息\n```java\nprivate final Map<String, VolumeFailureInfo> volumeFailureInfos = Collections.synchronizedMap(new TreeMap<String, VolumeFailureInfo>());\nprivate Object checkDirsMutex = new Object();\n```\n2. FSvolumeList.addVolumeFailureInfo()是添加失败卷方法的入口，在调用FSDatasetImpl.addVolume()时，在添加卷的时候如果发生异常，则把这个卷加到失败卷中，大致分为2种情况来调用这个方法\n- 第一种情况，在DN启动初始化块池的时候通过checkDiskError()对盘进行检查，将错误的盘添加到上述的列表中\n- 第二种情况，由其他操作出发的checkDiskError()，其他操作比如写数据块，directoryScanner扫描等操作，对于某个目录出现异常时会通过startCheckDiskErrorThread()启动一个异步线程，通过改变flag每5秒调用checkDiskError()，如果已启动则不会重新创建线程\n3. checkDiskError()逻辑如下\n- 依次检查每个盘，每个盘下的块池，块池下的三个目录从上到下递归\n- 通过创建文件夹等操作是否成功来判断有没有发生异常，如果发现一层目录出现问题则会递归回上层目录再进行一次判断\n- 将失败的目录添加到volumeFailureInfos的映射表中，并把这个目录从DN上删除\n- 构造ErrorReportAction将错误信息通过BPOfferServices.trySendErrorReport()进行处理，将errorcode和信息封装成action，放到一个异常队列里。这里的异常信息就是所有的坏盘路径\n- DN通过rpc调用errorRepor()，NN收到请求后根据code，如果这个DN此刻的盘数量大于可容忍的最小盘数量，则打印日志，否则认为DN已经挂\n\n## 2.发现磁盘损坏后是如何发送给NN的？\n- DataNode启动后会定时的发送心跳，发送心跳的时候会获取DataSet上的失败卷的信息，将其作为参数上传\n- NN收到rpc请求后，最终通过对应的DatanodeDescriptor.updateHeartbeat()来处理这次心跳\n- 每个对应的DatanodeDescriptor会将失败的盘的信息放入到VolumeFailureSummary，每次web请求webUI会从所有的DatanodeDescriptor中获取失败的盘的信息并展示\n\n## 3.如果一个盘坏了会造成什么影响？\n- 会被检测到，会将这个目录从DN中删除，包括存在DataStorage中对应的所有目录以及这个盘下的所有块池的目录\n- 当配置文件更新时，会重新设置配盘的目录，这个目录为当前有效的盘的信息，所以此时的坏盘将会一直被排除在外\n\n## 4.为什么原来需要重启才能使得新换的盘被识别？\n- 由于DN只提供一个方法，这个方法可以将定义的配置文件上的所有盘目录重新进行一次加载，但这个方法只是在配置文件改变时由这个线程进行调用。而且存在一个问题，及每次加载完成盘目录后，会重新设置盘的目录，重新设置的值为当前有效的所有卷，这就会导致所有的失败卷加载不进来直到下一次配置文件发生变动\n- 重启重新加载所有盘的路径所以没问题\n\n## 三、解决方法\n在DN初始化的时候启动一个守护线程，这个线程定时获取所有盘的路径，并刷新这些盘的路径，将有变动的并且是存活的盘添加到盘的列表中。具体逻辑可参照DN配置文件自动刷新线程\n\n## 四、如何测试\n### 1.测试环境介绍\n整个集群为一个HA结构，设置了3个DN，主要看DN的盘的配置，在默认的12个盘的基础上加了一个temp盘用来测试，此时这个DN和NN上并不存在坏盘的信息。\n\n```\n\"StorageInfo\" : \"FSDataset{dirpath='[/data1/hadoop/dfs/data/current, /data2/hadoop/dfs/data/current, /data3/hadoop/dfs/data/current, /data4/hadoop/dfs/data/current, /data5/hadoop/dfs/data/current, /data6/hadoop/dfs/data/current, /data7/hadoop/dfs/data/current, /data8/hadoop/dfs/data/current, /data9/hadoop/dfs/data/current, /data10/hadoop/dfs/data/current, /data11/hadoop/dfs/data/current, /data12/hadoop/dfs/data/current, \n  /opt/meituan/temp/current]'}\",\n\"Capacity\" : 44356908343296,\n\"DfsUsed\" : 2215936,\n\"CacheCapacity\" : 0,\n\"CacheUsed\" : 0,\n\"NumFailedVolumes\" : 0,\n\"FailedStorageLocations\" : [ ],\n```\n### 2.具体测试步骤\n#### a.测试前验证盘可用\n起始环境如图，这次操作主要在test01这个DN上，test01的temp盘用于测试，经过验证temp盘的读写均没有问题\n#### b.模拟坏盘\n将test01原先的temp盘目录权限改为600,发现坏盘被识别,这次识别主要是DirectoryScanner定时扫描，发现异常后调用的checkDiskError线程来检测到的，为了复现问题调低了每次检查的间隔时间\n```\n2019-07-26 17:21:27,372 WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Failed to write dfsUsed to /opt/meituan/temp/current/BP-2078753838-10.20.129.5-1557391954152/current/dfsUsed\njava.io.FileNotFoundException: /opt/meituan/temp/current/BP-2078753838-10.20.129.5-1557391954152/current/dfsUsed (Permission denied)\n        at java.io.FileOutputStream.open(Native Method)\n        at java.io.FileOutputStream.<init>(FileOutputStream.java:221)\n        at java.io.FileOutputStream.<init>(FileOutputStream.java:171)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.saveDfsUsed(BlockPoolSlice.java:243)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.shutdown(BlockPoolSlice.java:678)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.shutdown(FsVolumeImpl.java:1009)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.removeVolume(FsVolumeList.java:327)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList.checkDirs(FsVolumeList.java:249)\n        at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.checkDataDir(FsDatasetImpl.java:2005)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.checkDiskError(DataNode.java:3246)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode.access$1100(DataNode.java:251)\n        at org.apache.hadoop.hdfs.server.datanode.DataNode$8.run(DataNode.java:3279)\n        at java.lang.Thread.run(Thread.java:745)\n2019-07-26 17:21:27,372 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Removed volume: /opt/meituan/temp/current\n2019-07-26 17:21:27,373 WARN org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Completed checkDirs. Found 1 failure volumes.\n2019-07-26 17:21:27,373 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Deactivating volumes (clear failure=false): /opt/meituan/temp\n2019-07-26 17:21:27,373 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Removing /opt/meituan/temp from FsDataset.\n2019-07-26 17:21:27,373 INFO org.apache.hadoop.hdfs.server.common.Storage: Removing block level storage: /opt/meituan/temp/current/BP-2078753838-10.20.129.5-1557391954152\n```\n成功被DN检测到，成功通过心跳上传后，NN通过DataNodeDatanodeDescriptor拿到坏盘的信息并展示\n```\n2019-07-26 17:59:28,374 WARN org.apache.hadoop.hdfs.server.namenode.NameNode: Disk error on DatanodeRegistration(10.26.55.136:50010, datanodeUuid=ee82a609-5131-4014-8b3f-7b76b38b160d, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-0f757b26-eed8-4bf3-bb0c-d0cb6fb14c30;nsid=328516216;c=0): DataNode failed volumes:/opt/meituan/temp;\n2019-07-26 17:59:28,394 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 1\n2019-07-26 17:59:28,394 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: [DISK]DS-c50021d6-2526-4d69-93b2-539cd1be1d20:NORMAL:10.26.55.136:50010 failed.\n2019-07-26 17:59:28,394 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Removed storage [DISK]DS-c50021d6-2526-4d69-93b2-539cd1be1d20:FAILED:10.26.55.136:50010 from DataNode10.26.55.136:50010\n```\n一段时间后DN会把这个盘的存储的元数据信息完全删除，但存储的块任然在\n#### 坏盘恢复验证\n通过chmod将测试盘的权限改为777，一段时间后成功被修复，并且NN成功将其从坏盘的列表中删除，并且读写正常\n```\n2019-07-26 17:27:50,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: failVolumeScanner start work interval: 30000 volumesDir: [DISK]file:/data1/hadoop/dfs/data/,[DISK]file:/data2/hadoop/dfs/data/,[DISK]file:/data3/hadoop/dfs/data/,[DISK]file:/data4/hadoop/dfs/data/,[DISK]file:/data5/hadoop/dfs/data/,[DISK]file:/data6/hadoop/dfs/data/,[DISK]file:/data7/hadoop/dfs/data/,[DISK]file:/data8/hadoop/dfs/data/,[DISK]file:/data9/hadoop/dfs/data/,[DISK]file:/data10/hadoop/dfs/data/,[DISK]file:/data11/hadoop/dfs/data/,[DISK]file:/data12/hadoop/dfs/data/,/opt/meituan/temp\n2019-07-26 17:27:50,270 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Adding new volumes: [DISK]file:/opt/meituan/temp/\n2019-07-26 17:27:50,297 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /opt/meituan/temp/in_use.lock acquired by nodename 185696@gha-data-hdp-dn-test01.gh.sankuai.com\n2019-07-26 17:27:50,336 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /opt/meituan/temp/current/BP-2078753838-10.20.129.5-1557391954152\n2019-07-26 17:27:50,336 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.\n2019-07-26 17:27:50,352 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-c50021d6-2526-4d69-93b2-539cd1be1d20\n2019-07-26 17:27:50,352 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /opt/meituan/temp, StorageType: DISK\n2019-07-26 17:27:50,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully added volume: [DISK]file:/opt/meituan/temp/\n2019-07-26 17:27:50,353 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-2078753838-10.20.129.5-1557391954152 (Datanode Uuid ee82a609-5131-4014-8b3f-7b76b38b160d): scheduling a full block report.\n```\n## 五、结论\n上述方案可以有效的解决坏盘恢复后造成识别的问题，之后DN换盘后无需重启DN也能动态的将盘重新加载进DN。","tags":["hadoop"],"categories":["大数据"]},{"title":"实现Hdfs的几个功能","url":"/2018/12/10/2018-12-10/","content":"<Excerpt in index | 首页摘要>\n仿照hdfs实现了其几个功能\n<!-- more -->\n\n## 暂时简单实现了如下几个功能\n- RPC系统（整个系统的核心）\n- 操作日志的本地持久化（对比如创建目录等操作进行持久化记录）\n- 命名空间（namenode的文件目录树）\n- 客户端文件的上传和存储\n- 简单的管道写\n- 简单的租约机制\n\n接下来是对这几个模块方法的简单介绍\n## RPC系统（整个系统的核心）\n\n主要对Rpc监听对象进行注册、执行逻辑全在其对应的接口方法里\n``` java\npublic class NameNodeRpcServer implements NameNodeProtocols {\n\n    /** 这里要分为两个一个来监听namenode 另一个来监听datanode */\n    private RPC.Server serviceRpcServer = null;\n    BlockManager blockManager = new BlockManager();\n    protected final FSNamesystem namesystem = new FSNamesystem();\n    protected FSEditLog fsEditLog = null;\n    private BlockingService clientNNPbService = null;\n\n    public NameNodeRpcServer() {\n        ClientNamenodeProtocolTranslatorServer clientProtocolServerTranslator\n                = new ClientNamenodeProtocolTranslatorServer(this);\n        //构造ClientNamenodeProtocol BlockingServer对象，这个对象可以将service请求转到Translate对象这里\n        BlockingService clientNNPbService = ClientNamenodeProtocol.\n                newReflectiveBlockingService(clientProtocolServerTranslator);\n\n        serviceRpcServer = new RPC.Builder()\n                .setBindAddress(\"localhost\")\n                .setProtocol(ClientNamenodeProtocolPB.class)\n                .setInstance(clientNNPbService)\n                .setPort(7777)\n                .build();\n        fsEditLog = new FSEditLog();\n        //此处默认写死后期可以扩展\n        ArrayList<String> list = new ArrayList<>();\n        //操作日志存储的本地目录后期自定义\n        list.add(\"E://ddd.txt\");\n        fsEditLog.initJournal(list);\n        RpcserverThread thread = new RpcserverThread();\n        thread.start();\n    }\n\n    public class RpcserverThread extends Thread {\n        public void run() {\n            serviceRpcServer.start();\n        }\n    }\n    //暂未完成\n    @Override\n    public boolean rename(String a, String b) throws ServiceException {\n        System.out.println(\"now is doing----------\");\n        return false;\n    }\n\n    @Override\n    public boolean register(int id, String address, int port) {\n        InetSocketAddress add = new InetSocketAddress(address, port);\n        //map\n        namesystem.getRegister().put(id, add);\n        //arraylist\n        namesystem.getDatanodelist().add(id);\n        namesystem.getHeartbeat().put(id, System.currentTimeMillis());\n        return true;\n    }\n    //可用等待后期扩充\n    @Override\n    public int heart(int id) {\n        return 666;\n    }\n    //暂未完成\n    @Override\n    public LocatedBlocks getBlockLocations(String src, int offset, int length) throws ServiceException {\n//\n//        LocatedBlocks block = namesystem.getBlockLocations(src, \"name\", 1, 2);\n\n        return null;\n    }\n\n    //测试通过\n    //创建新的数据块的步骤1、先调用create 2、再调用addblock\n    //一个文件分成三个数据块传输\n    @Override\n    public LocatedBlock addblock(long length, String hdfspath, int head) throws ServiceException {\n        System.out.println(\"the addblockpath is \" + hdfspath);\n        System.out.println(\"now name node is add block\");\n        LocatedBlock additionalBlock = namesystem.getAdditionalBlock(hdfspath);\n        return additionalBlock;\n        //调用完毕后要namenoderpcServer.removelease\n    }\n\n    @Override\n    public boolean create(String src, String target) throws ServiceException {\n        //添加日志操作\n        fsEditLog.openForWrite();\n        //这里还有问题要把target做进一步的解析\n        System.out.println(\"success create|||||||||||||\");\n        fsEditLog.logAdd(\"/roote\", \"/success.txt\");\n        fsEditLog.flushrest();\n        //拿到一个datanode的address\n        ArrayList<Integer> datanodelist = namesystem.getDatanodelist();\n        if (datanodelist.isEmpty()) {\n            new RuntimeException(\"no datanode in list\");\n            try {\n                Thread.sleep(3333);\n            } catch (InterruptedException e) {\n                e.printStackTrace();\n            }\n        }\n        //将取出的DataNode放到队列的最后\n        Integer firstdatanode = datanodelist.get(0);\n        datanodelist.remove(firstdatanode);\n        datanodelist.add(firstdatanode);\n        InetSocketAddress address = namesystem.getRegister().get(firstdatanode);\n        //  文件名到blcokinfo\n        ConcurrentHashMap<String, BlockInfo> map = namesystem.getBlockMap();\n        BlockInfo blockInfo = new BlockInfo(address, 1, firstdatanode);\n        namesystem.startFile(target, blockInfo);\n        map.put(target, blockInfo);\n        return true;\n    }\n    //暂未完成\n    @Override\n    public String blockReport(String id, String block, String context) throws ServiceException {\n        //context存放datanode的具体IP和信息\n        String[] local = context.split(\",\");\n        String hostname = local[0];\n        int port = Integer.parseInt(local[1]);\n        String name = local[3];\n        InetSocketAddress address = new InetSocketAddress(hostname, port);\n        System.out.println(\"blockReport\");\n        BlockInfo block1 = new BlockInfo(address, Integer.parseInt(id), Integer.parseInt(id));\n        DatanodeStorageInfo datanodeStorageInfo = new DatanodeStorageInfo();\n        blockManager.addStoredBlock(block1, datanodeStorageInfo);\n        return \"success\";\n    }\n}\n```\n\n\n\n## 操作日志的本地持久化（对比如创建目录等操作进行持久化记录）\n\n最终通过这个类为入口、开始对操作日志进行持久化操作\n``` java\n\tpublic class FSEditLog {\n    //    private AtomicLong txid = new AtomicLong(1);\n    int id = 0;\n    //EditLogFileOutputStream\n    private EditLogOutputStream editLogStream = null;\n\n\n    private JournalSet journalSet = null;\n\n    public State state = State.UNINITIALIZED;\n\n    //传入URI的存储目录\n    //最终将edit文件存到第三方的介质上\n    public void initJournalsForWrite(List<String> uriList) {\n        if (uriList == null)\n            new RuntimeException(\"the uriList is empty\");\n        initJournal(uriList);\n        state = State.BETWEEN_LOG_SEGMENTS;\n    }\n\n    //\n    public void initJournal(List<String> uriList) {\n        //正常情况下配置文件加载\n        int minimumRedundantJournals = 10;\n        journalSet = new JournalSet(minimumRedundantJournals);\n        //StorageDirectory sd = storage.getStorageDirectory(u);\n        for (String uri : uriList) {\n            //把FileJournalManager进行包装为JournalAndStream最后存放在JournalSet中的journals中\n            //构造的FileJournalManager里面存储具体的存储信息\n            journalSet.add(new FileJournalManager(uri));\n        }\n    }\n\n    public void openForWrite() {\n        assert state != State.BETWEEN_LOG_SEGMENTS : \"the state is error in openforwrite\";\n        //返回最后一个transactionid作为本次的起始id\n        //异常判断是否有流包含新的这个起始id\n        startLogSeqment();\n    }\n\n    //开始构造流\n    public synchronized void startLogSeqment() {\n        editLogStream = journalSet.startLogSeqment();\n        state = State.IN_SEGMENT;\n    }\n\n    public void endCurrentLogSegment() {\n        assert state != State.IN_SEGMENT : \"the state is error in endCurrentLogSegment\";\n        long lastid = 0;\n    }\n\n    public void close() {\n\n    }\n\n    //输出流的写方法最终入口\n    public void logEdit(FSEditLogOp op) {\n        editLogStream.write(op);\n        if (!shouldSync()) {\n            return;\n        }\n        flushAndSync();\n    }\n\n    public void flushrest() {\n        editLogStream.flushres();\n    }\n\n    public synchronized void flushAndSync() {\n        editLogStream.flush();\n    }\n\n    public boolean shouldSync() {\n        return editLogStream.shouldSync();\n    }\n\n    //删除某个节点的日志\n    public void logDelete() {\n        FSEditLogOp op = null;\n        logEdit(op);\n    }\n\n    //添加某个节点的日志\n    public void logAdd(String src, String add) {\n//        FSEditLogOp op=new FSEditLogOp(txid.get(),src+\"#\"+add);\n//        txid.addAndGet(1);\n        FSEditLogOp op = new FSEditLogOp(id, src + \"#\" + add);\n        logEdit(op);\n    }\n}\n```\n## 命名空间（namenode的文件目录树）\n维护的整个命名空间的入口类只实现了部分功能\n```java\n\tpublic class FSNamesystem {\n    final LeaseManager leasemanager = new LeaseManager(this);\n    private ArrayList<Integer> datanodelist = new ArrayList<>();\n    ConcurrentHashMap<Integer, Long> heartbeat = new ConcurrentHashMap<>();\n    private ConcurrentHashMap<Integer, InetSocketAddress> register = new ConcurrentHashMap();\n    INodeFile lastiNodeFile = null;\n    //客户端的具体信息\n    ConcurrentHashMap<String, InetSocketAddress> clientmessage = new ConcurrentHashMap<>();\n    //文件名到blcokinfor\n    ConcurrentHashMap<String, BlockInfo> Blockmap = new ConcurrentHashMap();\n    //    //大块到每个小块\n    ConcurrentHashMap<String, INodeFile> inodemap = new ConcurrentHashMap<>();\n\n    //    ConcurrentHashMap<LocatedBlocks,LocatedBlock> blockmap = new ConcurrentHashMap();\n    final FSDirectory directory = new FSDirectory();\n\n    public BlockInfo getBlockLocations(String client, String src, long offset, long length) {\n//        InetSocketAddress address = clientmessage.get(client);\n        BlockInfo blcokinfo = Blockmap.get(src);\n        return blcokinfo;\n    }\n\n    //通过inodefile拿到blockInfo\n    public LocatedBlock getAdditionalBlock(String path) {\n        String[] split = path.split(\"/\");\n        boolean safe = leasemanager.checkLeases();\n        //轮询判断租约安全\n        if (!safe) {\n            while (leasemanager.checkLeases()) {\n                try {\n                    Thread.sleep(5555);\n                } catch (InterruptedException e) {\n                    e.printStackTrace();\n                }\n            }\n        }\n        if (!split[split.length - 1].equals(lastiNodeFile.name)) {\n            throw new RuntimeException(\"do not found block\");\n        }\n        //分配副本策略\n//        lastiNodeFile;\n        //  readLock();\n//        LocatedBlock locatedBlock = directory.getPathComponentsForReservedPath(path);\n        //分配副本策略\n        Integer index = datanodelist.get(0);\n        InetSocketAddress address = register.get(index);\n        //权限检查\n        //拿到上一个存储的block\n        //添加的时候根本没有加进去\n        BlockInfo block = lastiNodeFile.getBlocks();\n        if (block == null) {\n            new RuntimeException(\"can not get the block message \");\n        }\n        //构造返回的locatedBlock\n        LocatedBlock locatedBlock = new LocatedBlock(block.getAddress());\n        //添加租约\n        leasemanager.addLease(block.getdatanodeId() + \"\", path);\n        return locatedBlock;\n    }\n\n    public boolean removelease(int datanodeid) {\n        return leasemanager.removeleas(datanodeid);\n    }\n\n    void startLeaseServer() {\n        leasemanager.startMonitor();\n    }\n\n    void stopLeaseServer() {\n        leasemanager.stopMonitor();\n    }\n\n    public Boolean startFile(String path, BlockInfo blockInfo) {\n        lastiNodeFile = directory.addChild(path, blockInfo);\n        return true;\n    }\n\n \n\n    public ArrayList<Integer> getDatanodelist() {\n        return datanodelist;\n    }\n\n    public ConcurrentHashMap<Integer, InetSocketAddress> getRegister() {\n        return register;\n    }\n\n    public ConcurrentHashMap<String, InetSocketAddress> getclientmessage() {\n        return clientmessage;\n    }\n\n    public ConcurrentHashMap<String, BlockInfo> getBlockMap() {\n        return Blockmap;\n    }\n\n    public ConcurrentHashMap<String, INodeFile> getInodedir() {\n        return inodemap;\n    }\n\n    public void setInodedir(ConcurrentHashMap<String, INodeFile> inodedir) {\n        this.inodemap = inodedir;\n    }\n\n    public ConcurrentHashMap<Integer, Long> getHeartbeat() {\n        return heartbeat;\n    }\n}\n\n```\n## 客户端文件的上传和存储\n客户端获取输入流的入口类\n```java\npublic class DFSInputStream extends FSInputStream {\n    DFSClient dfsClient;\n    String src;\n    int buffersize;\n    boolean verifyChecksum;\n    //文件对应的所有数据块的信息\n    long lastBlockBeingWrittenLength;\n    //久的block信息\n    private DatanodeInfo currentNode = null;\n    private LocatedBlocks locatedBlocks = null;\n    BlockReader blockReader = null;\n    byte[] head = null;\n\n    public DFSInputStream(DFSClient client, String src, int buffersize, boolean verifyChecksum) {\n        this.dfsClient = client;\n        this.src = src;\n        this.buffersize = buffersize;\n        this.verifyChecksum = verifyChecksum;\n        //主要拿到上一个已经写的数据块的长度\n        openinfo();\n    }\n\n    //从namenode获取文件对应的数据块信息\n    public void openinfo() {\n        //获取文件对应的所有数据块的信息\n        lastBlockBeingWrittenLength = fetchLocatedBlocksAndGetLastBlockLength();\n    }\n\n    private long fetchLocatedBlocksAndGetLastBlockLength() {\n        //通过rpc数据块的位置信息\n        final LocatedBlocks newInfo = dfsClient.getLocatedBlocks(src, 0);\n        //将新的info和旧的info通过迭代器进行对比如果不同则跟新旧的\n        return 999;\n    }\n\n    ;\n\n    @Override\n    public void seek(long pos) throws IOException {\n\n    }\n\n    @Override\n    public long getPos() throws IOException {\n        return 0;\n    }\n\n    @Override\n    public boolean seekToNewSource(long targetPos) throws IOException {\n        return false;\n    }\n\n    @Override\n    public int read() throws IOException {\n        return 0;\n    }\n\n    \n    private static class ByteArrayStrategy implements ReaderStrategy {\n        final byte[] buf;\n\n        public ByteArrayStrategy(byte[] buf) {\n            this.buf = buf;\n        }\n\n        //绕了一大圈通过这个doRead来读取数据\n        @Override\n        public int doRead(BlockReader blockReader) {\n            blockReader.read();\n            return -1;\n        }\n    }\n\n    //包装不同的可能读取实现，以便readBuffer可以和策略无关。\n    private interface ReaderStrategy {\n        //        public int doRead(BlockReader blockReader, int off, int len,\n//                          ReadStatistics readStatistics) throws ChecksumException, IOException;\n        public int doRead(BlockReader blockReader);\n    }\n\n    //这里创建读取的策略！\n    public synchronized int read(final byte buf[], int off, int len) throws IOException {\n        ReaderStrategy byteArrayReader = new ByteArrayStrategy(buf);\n        return readWithStrategy(byteArrayReader, off, len);\n    }\n\n    private int readWithStrategy(ReaderStrategy strategy, int off, int len) throws IOException {\n        long pos = 1;\n        int time = 3;\n        while (time > 3) {\n            currentNode = blockSeekTo(pos);\n        }\n        //ByteArrayStrategy\n        readBuffer(strategy);\n        return 1;\n    }\n\n    //打开DataInputStream和datanode用来读取的通道、可以在启动时从namenode获取 block ID and the IDs\n    //获取保存下一个数据块的datanode\n    private synchronized DatanodeInfo blockSeekTo(long target) throws IOException {\n        while (true) {\n            //计算所需的块\n            LocatedBlock targetBlock = getBlockAt(target, true);\n            blockReader = new BlockReaderFactory().build();\n            return null;\n        }\n    }\n\n    //最终通过这个方法来读取数据\n    private synchronized int readBuffer(ReaderStrategy reader) {\n        //通过blockreader来读取数据、blockreader是内部的成员变量\n        reader.doRead(blockReader);\n        return 1;\n    }\n\n    //在指定的位置获取块。如果没有缓存，则从namenode获取它。\n    private synchronized LocatedBlock getBlockAt(long offset, boolean updatePosition) {\n        if (locatedBlocks == null)\n            System.out.println(\"error\");\n        return null;\n    }\n}\n```\n\n\n### 最近事比较多这里只是简单介绍下、以后会更详细的补上的、估计也没人看就先这样吧、\n### 整个系统依赖protobuf和netty库、入口方法为4个start方法、先启动server、再启动datanode、最后启动客户端\n","tags":["hadoop"],"categories":["大数据"]},{"title":"Hdfs RPC流程","url":"/2018/09/13/2018-9-13/","content":"<Excerpt in index | 首页摘要>\n先从最基本的RPC调用的源码开始着手\n<!-- more -->\n简单总结hdfs RPC执行的全部流程\n\n\n\n## RPC调用入口\n首先为了获得代理对象会执行RPC.getProxy()中的Proxy.newInstace()获得实例\n初始化这个实例时会初始化参数InvocationHandler这个类\n调用具体代理方法时会调用InvocationHandler里面已经重写过的invoke()方法\n这个方法里定义了与远程的链接通过这个方法便能获得远程调用后方法的返回值\n\n```java\n\n//启动客户端\npublic static void main(String[] args) throws Exception {\n\t\tClientNamenodeProtocol namenode = RPC.getProxy(ClientNamenodeProtocol.class, 1L,\n\t\t\t\tnew InetSocketAddress(\"localhost\", 9999), new Configuration());\n\t\tString metaData = namenode.getMetaData(\"/a.a\");\n\t\tSystem.out.println(metaData);\n\t}\n\n//服务端\npublic static void main(String[] args) throws Exception {\n\t\t//工厂方法、、、、\n\t\tBuilder builder = new RPC.Builder(new Configuration());\n\t\tbuilder.setBindAddress(\"localhost\")\n\t\t.setPort(9999)\n\t\t.setProtocol(ClientNamenodeProtocol.class)\n\t\t.setInstance(new MyNameNode());\n\t\tServer server = builder.build();\n\t\tserver.start();\n\t}\n\t\n```\n\nRPC.getProxy\n\n```java\n\npublic static <T> T getProxy(Class<T> protocol, long clientVersion,netSocketAddress addr, Configuration conf)\n     throws IOException {\n     return getProtocolProxy(protocol, clientVersion, addr, conf).getProxy();\n   }\n   \n//如果配置文件里没有配置返回一个默认的socketFactory\nSocketFactory getDefaultSocketFactory(){\n    \n}   \n```\n\n```java\n   \n//经过一系列调用得到RPC的引擎\nstatic synchronized RpcEngine getProtocolEngine(Class<?> protocol,\n      Configuration conf) {\n    RpcEngine engine = PROTOCOL_ENGINES.get(protocol);\n    if (engine == null) {\n      Class<?> impl = conf.getClass(ENGINE_PROP+\".\"+protocol.getName(),\n                                    WritableRpcEngine.class);\n      //用反射工具类实例化一个对象imp是这个引擎的全限定名，conf是配置文件 6个xml文件\n      engine = (RpcEngine)ReflectionUtils.newInstance(impl, conf);\n      //此处protocol为一开始传入的接口的全限定名\n      PROTOCOL_ENGINES.put(protocol, engine);\n    }\n    return engine;\n  }\n  \n//拿到引擎后开始调用引擎的getProxy方法\n//在writable引擎中\n//tag1----\npublic <T> ProtocolProxy<T> getProxy(){\n    //开始动态代理\n    T proxy = (T)\n    //拿到协议的类加载器\n    Proxy.newProxyInstance(protocol.getClassLoader(),new Class[] { protocol }, new Invoker(protocol, addr, ticket, conf,factory, rpcTimeout, fallbackToSimpleAuth));\n    //上方起跳\n    return new ProtocolProxy<T>(protocol, proxy,  true);\n}\n//拿到引擎后开始调用引擎的getProxy方法\n//在writable引擎中\n//getProxy方法中的动态代理拿到中的参数要实例化Invoker\n//在构造方法中主要初始化Invoker成员变量\n//重要的有Client\n//这个内部类实现了RpcInvocationHandler这个接口\n//ConnectionId是Client的静态内部类事先已经导入\n//这个类保存地址和用户ticket。客户端连接\n//服务器的唯一标识是<remoteAddress, protocol, ticket> ticket的一个例子“123,auth:SIMPLE”\n public Invoker(Class<?> protocol,\n                   InetSocketAddress address, UserGroupInformation ticket,\n                   Configuration conf, SocketFactory factory,\n                   int rpcTimeout, AtomicBoolean fallbackToSimpleAuth)\n        throws IOException {\n      this.remoteId = ConnectionId.getConnectionId(address, protocol,\n          ticket, rpcTimeout, conf);\n      this.client = CLIENTS.getClient(conf, factory);\n      this.fallbackToSimpleAuth = fallbackToSimpleAuth;\n    }\n//拿到引擎后开始调用引擎的getProxy方法\n//在writable引擎中\n//getProxy方法中的动态代理拿到中的参数要实例化Invoker\n//在构造方法中主要初始化Invoker成员变量\n//初始化成员变量时要通过getClient来获得client\n\n\n//getClient会从这里面的map中拿到getClient()方法key是socket Factory\npublic class ClientCache {\n  private Map<SocketFactory, Client> clients = new HashMap<SocketFactory, Client>();\n\n //第一次的入口\n public synchronized Client getClient(Configuration conf, SocketFactory factory) {\n    return this.getClient(conf, factory, ObjectWritable.class);\n  }\n\n\n public synchronized Client getClient(Configuration conf,\n      SocketFactory factory, Class<? extends Writable> valueClass) {\n    Client client = clients.get(factory);\n    //没有的话造一个Client第一次的话是没有的\n    if (client == null) {\n      client = new Client(valueClass, conf, factory);\n      clients.put(factory, client);\n    } else {\n      client.incCount();\n    }\n    if (Client.LOG.isDebugEnabled()) {\n      Client.LOG.debug(\"getting client out of cache: \" + client);\n    }\n    return client;\n  }\n\n}\n\n```\n\n```java\n//拿到引擎后开始调用引擎的getProxy方法\n//在writable引擎中\n//getProxy方法中的动态代理拿到中的参数要实例化Invoker\n//在构造方法中主要初始化Invoker成员变量\n//初始化成员变量时要通过getClient来获得client\n//拿到Client后继续初始化成员变量\n//现在在writable的引擎内部Invoker类中\n//初始化完成后回到这里\n\n\n//tag1----对应上方\npublic <T> ProtocolProxy<T> getProxy(){\n    //开始动态代理\n    T proxy = (T)\n    //拿到协议的类加载器\n    Proxy.newProxyInstance(protocol.getClassLoader(),new Class[] { protocol }, new Invoker(protocol, addr, ticket, conf,factory, rpcTimeout, fallbackToSimpleAuth));\n    //上方起跳\n    return new ProtocolProxy<T>(protocol, proxy,  true);\n}\n\n//这是一个另外独立的类\n//类包围了服务器的代理，\n//包含其支持的方法列表。\n//一个值为null的方法列表来表示客户端和服务器有相同的协议。\npublic class ProtocolProxy<T> {\n    //接口的全限定名\n    private Class<T> protocol;\n    private T proxy;\n    private HashSet<Integer> serverMethods = null;\n    //构造方法初始化变量\n    public ProtocolProxy(Class<T> protocol, T proxy,\n      boolean supportServerMethodCheck) {\n    this.protocol = protocol;\n    //这里会对其报错但不影响RuntimeException\n    this.proxy = proxy;\n    this.supportServerMethodCheck = supportServerMethodCheck;\n  }\n\n}\n\n//new 完ProtocolProxy并返回对象然后回到一开始的地方\npublic static <T> T getProxy(Class<T> protocol, long clientVersion,netSocketAddress addr, Configuration conf)\n     throws IOException {\n     return getProtocolProxy(protocol, clientVersion, addr, conf).getProxy(.......);\n   }\n   \n//现在回到程序的入口开始执行这条语句\n\tString metaData = namenode.getMetaData(\"/a.a\");\n\t\n//调用getMetaData方法时会调用writable的invoker内部类中的invoke方法\npublic Object invoke(Object proxy, Method method, Object[] args)\n      throws Throwable {\n      long startTime = 0;\n      if (LOG.isDebugEnabled()) {\n        startTime = Time.now();\n      }\n      TraceScope traceScope = null;\n      //这个方法存疑。。。\n      if (Trace.isTracing()) {\n        traceScope = Trace.startSpan(\n            method.getDeclaringClass().getCanonicalName() +\n            \".\" + method.getName());\n        //getName直接跳到下面\n        //得到非常完整的公有、全限定名、方法名、参数\n//tag2--------------------\n      }\n      ObjectWritable value;\n      try {\n//tag3--------------------\n        value = (ObjectWritable)\n          client.call(RPC.RpcKind.RPC_WRITABLE, new Invocation(method, args),\n            remoteId, fallbackToSimpleAuth);\n      } finally {\n        if (traceScope != null) traceScope.close();\n      }\n      if (LOG.isDebugEnabled()) {\n        long callTime = Time.now() - startTime;\n        LOG.debug(\"Call: \" + method.getName() + \" \" + callTime);\n      }\n      return value.get();\n    }\n\t\n```\n\n```java\n\n    \n//又进入了引擎的静态内部类\n private static class Invocation implements Writable, Configurable {\n    private String methodName;\n    private Class<?>[] parameterClasses;\n    private Object[] parameters;\n    private Configuration conf;\n    private long clientVersion;\n    private int clientMethodsHash;\n    private String declaringClassProtocolName;\n    \n    //构造方法\n    public Invocation(Method method, Object[] parameters) {\n      this.methodName = method.getName();\n      this.parameterClasses = method.getParameterTypes();\n      this.parameters = parameters;\n      rpcVersion = writableRpcVersion;\n      if (method.getDeclaringClass().equals(VersionedProtocol.class)) {\n        //VersionedProtocol is exempted from version check.\n        clientVersion = 0;\n        clientMethodsHash = 0;\n      } else {\n        this.clientVersion = RPC.getProtocolVersion(method.getDeclaringClass());\n        this.clientMethodsHash = ProtocolSignature.getFingerprint(method\n            .getDeclaringClass().getMethods());\n      }\n      //如果协议类具有ProtocolAnnotation，则获取协议注释中的名称;否则，类名就是协议名。暂时不明白作用\n      //同之前赋予了非常详细的方法名。。。\n      this.declaringClassProtocolName = \n          RPC.getProtocolName(method.getDeclaringClass());\n    }\n    \n}\n\n//重新回到tag2----开始tag3----\n//client中的call\n public Writable call(RpcKind rpcKind, Writable rpcRequest,\n      ConnectionId remoteId, AtomicBoolean fallbackToSimpleAuth)\n      throws IOException {\n    return call(rpcKind, rpcRequest, remoteId, RPC.RPC_SERVICE_CLASS_DEFAULT,\n      fallbackToSimpleAuth);\n  }\n\n//继续在类里面重载\npublic Writable call(RpcKind rpcKind, Writable rpcRequest,\n      ConnectionId remoteId, int serviceClass,\n      AtomicBoolean fallbackToSimpleAuth) throws IOException {\n//tag4---------------------开始跳\n    final Call call = createCall(rpcKind, rpcRequest);\n//tag5--------------开始跳\n    Connection connection = getConnection(remoteId, call, serviceClass,\n      fallbackToSimpleAuth);\n    try {\n      connection.sendRpcRequest(call);                 // send the rpc request\n    } catch (RejectedExecutionException e) {\n      throw new IOException(\"connection has been closed\", e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n      LOG.warn(\"interrupted waiting to send rpc request to server\", e);\n      throw new IOException(e);\n    }\n\n//tag4---------------------\nCall createCall(RpcKind rpcKind, Writable rpcRequest) {\n//创建一个内部类Call\n//Writable rpcRequest封装了方法的名字、client版本还有有关方法的一组数据。。。存疑\n    return new Call(rpcKind, rpcRequest);\n  }\n\n//回到tag4---\n```\n\n```java\n\n//进入tag5\n//Get a connection from the pool, or create a new one and add it to the pool.相同的ID可以重用\n\n//是Client类中的方法\n//ConnectionId remoteId远程的ip和端口\n//Call call 之前创建的一个类封装了方法名和参数\nprivate Connection getConnection(ConnectionId remoteId,\n      Call call, int serviceClass, AtomicBoolean fallbackToSimpleAuth)\n      throws IOException {\n    if (!running.get()) {\n      // the client is stopped\n      throw new IOException(\"The client is stopped\");\n    }\n    Connection connection;\n    /* we could avoid this allocation for each RPC by having a  \n     * connectionsId object and with set() method. We need to manage the\n     * refs for keys in HashMap properly. For now its ok.\n     */\n    do {\n      synchronized (connections) {\n        connection = connections.get(remoteId);\n        if (connection == null) {\n//tag6---------\n          connection = new Connection(remoteId, serviceClass);\n          connections.put(remoteId, connection);\n        }\n      }\n    //tag7----------------\n    } while (!connection.addCall(call));\n    \n    //we don't invoke the method below inside \"synchronized (connections)\"\n    //block above. The reason for that is if the server happens to be slow,\n    //it will take longer to establish a connection and that will slow the\n    //entire system down.\n    //tag8----------------------\n    connection.setupIOstreams(fallbackToSimpleAuth);\n    return connection;\n  }\n\n//tag6进入-------------------\n//非常重要的client的内部类reactor模式\nprivate class Connection extends Thread {\n        public Connection(ConnectionId remoteId, int serviceClass) throws IOException {\n      //第一次进来为null\n      this.remoteId = remoteId;\n      this.server = remoteId.getAddress();\n      if (server.isUnresolved()) {\n        throw NetUtils.wrapException(server.getHostName(),\n            server.getPort(),\n            null,\n            0,\n            new UnknownHostException());\n      }\n      this.rpcTimeout = remoteId.getRpcTimeout();\n      this.maxIdleTime = remoteId.getMaxIdleTime();\n      this.connectionRetryPolicy = remoteId.connectionRetryPolicy;\n      this.maxRetriesOnSasl = remoteId.getMaxRetriesOnSasl();\n      this.maxRetriesOnSocketTimeouts = remoteId.getMaxRetriesOnSocketTimeouts();\n      this.tcpNoDelay = remoteId.getTcpNoDelay();\n      this.doPing = remoteId.getDoPing();\n      //先Ping？有意思。。。。。。。true\n//------------下方高能底层操作以后慢慢来\n      if (doPing) {\n        // construct a RPC header with the callId as the ping callId\n        pingRequest = new ByteArrayOutputStream();\n        RpcRequestHeaderProto pingHeader = ProtoUtil\n            .makeRpcRequestHeader(RpcKind.RPC_PROTOCOL_BUFFER,\n                OperationProto.RPC_FINAL_PACKET, PING_CALL_ID,\n                RpcConstants.INVALID_RETRY_COUNT, clientId);\n        pingHeader.writeDelimitedTo(pingRequest);\n      }\n     // how often sends ping to the server in msecs\n     //60000\n      this.pingInterval = remoteId.getPingInterval();\n      this.serviceClass = serviceClass;\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"The ping interval is \" + this.pingInterval + \" ms.\");\n      }\n//\"123,Sim\"和上方的一样的ticket还有RemoteId这个类\n      UserGroupInformation ticket = remoteId.getTicket();\n      //高能操作----SASL全称Simple Authentication and Security Layer，是一种用来扩充C/S模式验证能力的机制。在Postfix可以利用SASL来判断用户是否有权使用转发服务，或是辨认谁在使用你的服务器\n      // try SASL if security is enabled or if the ugi contains tokens.\n      // this causes a SIMPLE client with tokens to attempt SASL\n      boolean trySasl = UserGroupInformation.isSecurityEnabled() ||\n                        (ticket != null && !ticket.getTokens().isEmpty());\n      this.authProtocol = trySasl ? AuthProtocol.SASL : AuthProtocol.NONE;\n      \n      this.setName(\"IPC Client (\" + socketFactory.hashCode() +\") connection to \" +\n          server.toString() +\n          \" from \" + ((ticket==null)?\"an unknown user\":ticket.getUserName()));\n      this.setDaemon(true);\n    }\n}\n\n//tag6-------------完成回到一开始的tag6\n\n\n//tag7-------------进入\n\n /**\n     * Add a call to this connection's call queue and notify\n     * a listener; synchronized.\n     * Returns false if called during shutdown.\n     * @param call to add\n     * @return true if the call was added.\n     */\n    private synchronized boolean addCall(Call call) {\n      if (shouldCloseConnection.get())\n        return false;\n        // currently active calls\n      calls.put(call.id, call);\n      notify();\n      return true;\n    }\n//tag7-------------退出\n\n```\n\n```java\n\n//tag8-------------进入\n private synchronized void setupIOstreams(\n        AtomicBoolean fallbackToSimpleAuth) {\n      if (socket != null || shouldCloseConnection.get()) {\n        return;\n      } \n      try {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Connecting to \"+server);\n        }\n        if (Trace.isTracing()) {\n          Trace.addTimelineAnnotation(\"IPC client connecting to \" + server);\n        }\n        short numRetries = 0;\n        Random rand = null;\n        while (true) {\n          setupConnection();\n          InputStream inStream = NetUtils.getInputStream(socket);\n          OutputStream outStream = NetUtils.getOutputStream(socket);\n          writeConnectionHeader(outStream);\n          if (authProtocol == AuthProtocol.SASL) {\n            final InputStream in2 = inStream;\n            final OutputStream out2 = outStream;\n            UserGroupInformation ticket = remoteId.getTicket();\n            if (ticket.getRealUser() != null) {\n              ticket = ticket.getRealUser();\n            }\n            try {\n              authMethod = ticket\n                  .doAs(new PrivilegedExceptionAction<AuthMethod>() {\n                    @Override\n                    public AuthMethod run()\n                        throws IOException, InterruptedException {\n                      return setupSaslConnection(in2, out2);\n                    }\n                  });\n            } catch (Exception ex) {\n              authMethod = saslRpcClient.getAuthMethod();\n              if (rand == null) {\n                rand = new Random();\n              }\n              handleSaslConnectionFailure(numRetries++, maxRetriesOnSasl, ex,\n                  rand, ticket);\n              continue;\n            }\n            if (authMethod != AuthMethod.SIMPLE) {\n              // Sasl connect is successful. Let's set up Sasl i/o streams.\n              inStream = saslRpcClient.getInputStream(inStream);\n              outStream = saslRpcClient.getOutputStream(outStream);\n              // for testing\n              remoteId.saslQop =\n                  (String)saslRpcClient.getNegotiatedProperty(Sasl.QOP);\n              LOG.debug(\"Negotiated QOP is :\" + remoteId.saslQop);\n              if (fallbackToSimpleAuth != null) {\n                fallbackToSimpleAuth.set(false);\n              }\n            } else if (UserGroupInformation.isSecurityEnabled()) {\n              if (!fallbackAllowed) {\n                throw new IOException(\"Server asks us to fall back to SIMPLE \" +\n                    \"auth, but this client is configured to only allow secure \" +\n                    \"connections.\");\n              }\n              if (fallbackToSimpleAuth != null) {\n                fallbackToSimpleAuth.set(true);\n              }\n            }\n          }\n        \n          if (doPing) {\n            inStream = new PingInputStream(inStream);\n          }\n          this.in = new DataInputStream(new BufferedInputStream(inStream));\n\n          // SASL may have already buffered the stream\n          if (!(outStream instanceof BufferedOutputStream)) {\n            outStream = new BufferedOutputStream(outStream);\n          }\n          this.out = new DataOutputStream(outStream);\n          \n          writeConnectionContext(remoteId, authMethod);\n\n          // update last activity time\n          touch();\n\n          if (Trace.isTracing()) {\n            Trace.addTimelineAnnotation(\"IPC client connected to \" + server);\n          }\n\n          // start the receiver thread after the socket connection has been set\n          // up\n          start();\n          return;\n        }\n      } catch (Throwable t) {\n        if (t instanceof IOException) {\n          markClosed((IOException)t);\n        } else {\n          markClosed(new IOException(\"Couldn't set up IO streams\", t));\n        }\n        close();\n      }\n    }\n\n```\n\n```java\n\n//tag8-------------退出\n```\n\nnamenode启动时会在初始化方法中创造NameNodeRpcServer对象的实例 这个实例对象构造函数会通过创建RPC.Builder内部类的静态实例\n\n```java\n\nthis.clientRpcServer = new RPC.Builder(conf)\n        .setProtocol(\n            org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB.class)\n        .setInstance(clientNNPbService).setBindAddress(bindHost)\n        .setPort(rpcAddr.getPort()).setNumHandlers(handlerCount)\n        .setVerbose(false)\n        .setSecretManager(namesystem.getDelegationTokenSecretManager()).build();\n\n```\n\n\n\n\n\n","tags":["hadoop"],"categories":["大数据"]},{"title":"安全检测","url":"/2018/05/03/2018-5-3 /","content":"<Excerpt in index | 首页摘要>\n某比赛要求在施工通过监控对没带安全帽的人进行报警\n<!-- more -->\n先吐槽一下比赛的主办方、给的测试视屏画质极低拍摄极为敷衍、有些人连人眼都无法识别是否带了安全帽、这小小的比赛大概整了整个51假期吧、\n## 简单介绍\n这里主要提供一下思路、传统ssd(高配电脑fater-rcnn走起)+inception3、你可能会问为什么不直接用ssd进行二次训练就好了、我当初也是这么想的这不是很简单么、\n然后我先把视频一帧帧的读取并转化成图像然后手动lable(这里有个问题就是一个图像中有多个人这样训练的时候会不会造成无法收敛？我觉得会有很大的影响)、\n然后训练这个像打了码一样的图片(再次吐槽一下主办方)、结果连人都识别不出来！！！内心极度奔溃、然后就用了独创非主流方法\n## 具体步骤(非主流方法请勿模仿、)\n鉴于之前连人都识别出来的问题、我就直接调用ssd先去除人、然后对有戴和没戴安全帽的进行训练(通过inception3)、然后运行通过ssd的目标检测结果输入到inception3中进行判别\n判别的结果传给之前的显示字符串然后进行输出、下面附上源码(目录与object_detection一致)\n\n```python\n#视频的读取得到识别物体后显示出来\nimport os\nimport cv2\nimport time\nimport numpy as np\nimport tensorflow as tf\n\nfrom utils.app_utils import FPS\nfrom object_detection.utils import label_map_util\nfrom object_detection.utils import visualization_utils as vis_util\n\nCWD_PATH = os.getcwd()\n\nMODEL_NAME = 'ssd_mobilenet_v1_coco_11_06_2017'\nPATH_TO_CKPT = os.path.join(CWD_PATH, 'object_detection', MODEL_NAME, 'frozen_inference_graph.pb')\nPATH_TO_LABELS = os.path.join(CWD_PATH, 'object_detection', 'data', 'mscoco_label_map.pbtxt')\n\nNUM_CLASSES = 2\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES,\n                                                            use_display_name=True)\n\ncategory_index = label_map_util.create_category_index(categories)\n\ndef detect_objects(image_np, sess, detection_graph):\n    # 增加输入图像的维度: [1, None, None, 3]\n    image_np_expanded = np.expand_dims(image_np, axis=0)\n    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n    # 得到检测框\n    boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n    #得到他的得分\n    scores = detection_graph.get_tensor_by_name('detection_scores:0')\n    classes = detection_graph.get_tensor_by_name('detection_classes:0')\n    num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n    # Actual detection.\n    # 这里的class是包含多个识别种类的二维数组\n    #[[100,4]]boxes 每个框的位置坐标,    scores 100个 ,     classes 100个 ,    num_detections 100个\n    (boxes, scores, classes, num_detections) = sess.run(\n        [boxes, scores, classes, num_detections],\n        feed_dict={image_tensor: image_np_expanded})\n    # Visualization of the results of a detection.\n    vis_util.visualize_boxes_and_labels_on_image_array(\n        image_np,\n        np.squeeze(boxes),\n        np.squeeze(classes).astype(np.int32),\n        np.squeeze(scores),\n        category_index,\n        use_normalized_coordinates=True,\n        line_thickness=4,\n        min_score_thresh=0.5)\n    return image_np\n\nif __name__ == '__main__':\n    detection_graph = tf.Graph()\n    with detection_graph.as_default():\n        od_graph_def = tf.GraphDef()\n        with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n            serialized_graph = fid.read()\n            od_graph_def.ParseFromString(serialized_graph)\n            tf.import_graph_def(od_graph_def, name='')\n    sess = tf.Session(graph=detection_graph)\n    video_capture = cv2.VideoCapture('b.mp4')\n    fps = FPS().start()\n    frame_width = int(video_capture.get(3))\n    frame_height = int(video_capture.get(4))\n    # define video output\n    out = cv2.VideoWriter('outpy.mp4', cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'), 10, (frame_width, frame_height))\n    count = 0\n    while video_capture.isOpened():\n        ret, frame = video_capture.read()\n        t = time.time()\n        detected_image = detect_objects(frame, sess, detection_graph)\n        fps.update()\n        cv2.imshow('Video', detected_image)\n\t\t#本来想来做个更加流畅的优化、就是格一个帧进行识别、但还是会阻塞\n        #if count % 100 == 0:\n        #    print(count)\n        # write to video file\n        #out.write(detected_image)\n        # print('[INFO] elapsed time: {:.2f}'.format(time.time() - t))\n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n\n    fps.stop()\n    video_capture.release()\n    sess.close()\n    cv2.destroyAllWindows()\n\n```\n```python\n#visualization_untils\n#第160行进行如下修改、check为inception3的入口、将图片和坐标传入\n  if use_normalized_coordinates:\n    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n                                  ymin * im_height, ymax * im_height)\n    \n    name=check(image.copy(), left, right, top, bottom)\n\t\n\t\n##188 行处\t\n#name为全局变量、接受inception3识别结果的字符串\ndraw.text(\n        (left + margin, text_bottom - text_height - margin),\n        name,\n        fill='black',\n        font=font)\n\n```\n\n```python\n\n#check模块、inception3的入口\nimport tensorflow as tf\nimport numpy as np\nfrom pylab import array\n\ndef check(image,left, right, top, bottom):\n    got = array(image)\n    crop_img = got[int(top):int(bottom), int(left):int(right), 0:3]\n\t#载入之前自己训练的模型\n    with tf.gfile.FastGFile('output_graph.pb', 'rb') as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n        tf.import_graph_def(graph_def, name='')\n\n    with tf.Session() as sess:\n        softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')\n\t\t#将传入的图片格式转化一下\n        first = tf.image.convert_image_dtype(crop_img, dtype=tf.float32)\n        # jpeg 进行编码\n        # eval()想当于将tensorflow的存储格式中提取出来以数组的格式\n        encode = tf.image.encode_jpeg(first.eval())\n        #将编码好的图片传入以decodejpeg的格式\n        predictions = sess.run(softmax_tensor, {'DecodeJpeg/contents:0': encode.eval()})  # 图片格式是jpeg格式\n        predictions = np.squeeze(predictions)  # 把结果转为1维数据\n        top_k = predictions.argsort()[::-1]\n        if top_k[0]==1:\n            human_string=\"unsafe\"\n        else:\n            human_string=\"safe\"\n        return human_string\n        #返回给画框的代码\n```\n\n## 总结\n看似十分完美流程的过程在实际运行时由于笔记本配置低下(好想要GPU的台式机！！)、换了一台配置稍微高一点的本、但还是崩了、tensorflow开两个session的内存消耗比想象中的要大、开\n看来这操作只能是活在梦里了、希望以后能想出一种底层之间的优化(相比之前的已经做了很多IO的优化、但主要问题还是这是线性的操作、一定有卡顿来进行二次判断)\n\n## 更新！！！\n终于找到了问题所在！！原来每一帧的图像传入后都要重新加载一次graph！！所以导致内存直接爆炸！改动后可以跑的动了、但比较吃配置配置高一点的话可以更加流畅吧、\n具体改动如下、其余的改动就是要在每个调用的visualization_utils中的函数里传入初始化的graph、具体修改如下、整个项目会放到github上\n\n```python\n#主要是对main函数下的修改vediondetection.py\n\nif __name__ == '__main__':\n    #tf.Graph()生成新的图\n    detection_graph = tf.Graph()\n    inceptionsess =tf.Graph()\n    with inceptionsess.as_default():\n        od_graph_def = tf.GraphDef()\n        with tf.gfile.FastGFile('output_graph.pb', 'rb') as f:\n            serialized_graph = f.read()\n            od_graph_def.ParseFromString(serialized_graph)\n            tf.import_graph_def(od_graph_def, name='')\n\n    with detection_graph.as_default():\n        od_graph_def = tf.GraphDef()\n        with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n            serialized_graph = fid.read()\n            od_graph_def.ParseFromString(serialized_graph)\n            tf.import_graph_def(od_graph_def, name='')\n    sess = tf.Session(graph=detection_graph)\n    video_capture = cv2.VideoCapture('b.mp4')\n    fps = FPS().start()\n    frame_width = int(video_capture.get(3))\n    frame_height = int(video_capture.get(4))\n    # define video output\n    out = cv2.VideoWriter('outpy.mp4', cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'), 10, (frame_width, frame_height))\n    count = 0\n    while video_capture.isOpened():\n        ret, frame = video_capture.read()\n        t = time.time()\n        detected_image = detect_objects(frame, sess, detection_graph,inceptionsess)\n        fps.update()\n        out.write(detected_image)\n        cv2.imshow('Video', detected_image)\n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n    fps.stop()\n    video_capture.release()\n    sess.close()\n    cv2.destroyAllWindows()\n```\n\n\n```python\n#对checker类的方法进行的改动\n\ndef check(image,left, right, top, bottom,inceptionsess):\n    got = array(image)\n    crop_img = got[int(top):int(bottom), int(left):int(right), 0:3]\n    # with tf.gfile.FastGFile('output_graph.pb', 'rb') as f:\n    #     graph_def = tf.GraphDef()\n    #     graph_def.ParseFromString(f.read())\n    #     tf.import_graph_def(graph_def, name='')\n    with tf.Session(graph=inceptionsess) as sess:\n        softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')\n        # jpeg 进行编码\n        # \"\"\"Return the value of the tensor represented by this handle.\"\"\n        encode = tf.image.encode_jpeg(crop_img)\n        predictions = sess.run(softmax_tensor, {'DecodeJpeg/contents:0': encode.eval()})  # 图片格式是jpg格式\n        predictions = np.squeeze(predictions)  # 把结果转为1维数据\n        top_k = predictions.argsort()[::-1]\n        if top_k[0]==1:\n            human_string=\"unsafe\"\n        else:\n            human_string=\"safe\"\n        return human_string\n\n```\n\n","tags":["图像识别"],"categories":["机器学习"]},{"title":"定点识别","url":"/2018/04/08/2018-4-8/","content":"<Excerpt in index | 首页摘要>\n基于object_detection训练自己的模型\n<!-- more -->\n花了不知道多少天、、主要参加一个定点识别的比赛、算是把模型搞定了、虽然结果十分的令人喜感（哈哈、不说了）、、难度有一点大（主要是各种天坑、在这里记录一下）\n\n这是阿里天池的比赛、比赛给出上万张图片主要是服装、要在每个图片上识别出服装每个关键点、并将识别结果的坐标输出、比如左袖口什么的、差不多有24个标签吧、训练集给出的是每个图片的所有关键点的坐标、我的思路是先根据坐标\n转化成矩形框(同时对x和y加上自己定义的距离数)、然后通过object_detection确定定位的位置、最后在进行输出(求两个x和两个y的平均来得到中心点)、具体步骤如下：\n\n## 根据lable切分图片\n\n这个脚本主要是根据lable对图片进行切分、根据lable创建若干个文件夹、切好的图片放到每个对应的文件加下、切分完得到几十万张图片(此刻的内心是奔溃的)、\n```python\nimport csv\nimport cv2\nimport os\n\npath=os.getcwd()\n#自己定义框的宽度wide\ndef drawcnts_and_cut(original_img,x,y,wide):\n    x1=x-wide\n    x2=x+wide\n    y1=y-wide\n    y2=y+wide\n    crop_img = original_img[y1:y2, x1:x2]\n    return  crop_img\n\ndef start(img_path,save_path,x,y):\n    original_img= cv2.imread(img_path)\n    crop_img = drawcnts_and_cut(original_img,int(x),int(y),25)\n    cv2.imwrite(save_path, crop_img)\ndef datatranslate(data):\n    splited=str(data).split()\n    return splited[0],splited[1]\n\t\n#自己根据标签数量来改\nlable=['class1', 'class2']\t\ncsv_reader = csv.reader(open('train\\\\input.csv', encoding='utf-8'))\nnum=0\nfor row in csv_reader:\n    for i in range(2,26,1):\n        photo=row[0]\n        data=row[i]\n        category=lable[i]\n        splited = str(row[i]).split(\"_\")\n        print(photo)\n        print(num)\n        if int(splited[0])!=-1:\n            lib = path + \"\\\\train\\\\\"+photo\n            savepath=path+\"\\\\output\\\\\"+str(category)+\"\\\\\"+str(category)+\"+\"+str(num)+\".jpg\"\n            num+=1\n            start(lib,savepath,splited[0],splited[1])\n\t\t\t\n```\n\n\n## 将图片转化为对应的xml文件\n\n默认的边框大小为整个图片的d、长度和宽度可以从图片中获取、最终批量的生成xml文件（突然想起比赛的图片切分后生成的30万个文件、还只能分批次的复制、一复制就卡屏、迷醉、、）\n```python\nimport os, sys\nimport glob\nfrom PIL import Image\n\n#根据实际来添加class\nlist=[\"class1\",\"class2\"]\nfor a in list:\n    path=os.getcwd()\n    #图像存储位置\n    src_img_dir = path+\"\\\\input2\\\\\"+a\n    # xml文件存放位置\n    src_xml_dir = path+\"\\\\input2\\\\\"+a\n    img_Lists = glob.glob(src_img_dir + '\\*.jpg')\n    img_basenames = [] \n    for item in img_Lists:\n        img_basenames.append(os.path.basename(item))\n    img_names = [] \n    for item in img_basenames:\n        temp1, temp2 = os.path.splitext(item)\n        img_names.append(temp1)\n    for img in img_names:\n        im = Image.open((src_img_dir + '/' + img + '.jpg'))\n        width, height = im.size\n        xml_file = open((src_xml_dir + '/' + img + '.xml'), 'w')\n        xml_file.write('<annotation>\\n')\n        xml_file.write('    <folder>'+a+'</folder>\\n')\n        xml_file.write('    <filename>' + str(img) + '.jpg' + '</filename>\\n')\n        xml_file.write('    <path>' + path +\"\\\\input2\\\\\"+a+\"\\\\\"+ str(img) + '.jpg'+ '</path>\\n')\n        xml_file.write('    <size>\\n')\n        xml_file.write('        <width>' + str(width) + '</width>\\n')\n        xml_file.write('        <height>' + str(height) + '</height>\\n')\n        xml_file.write('        <depth>3</depth>\\n')\n        xml_file.write('    </size>\\n')\n        xml_file.write('        <segmented>0</segmented>\\n')\n        xml_file.write('    <object>\\n')\n        xml_file.write('        <name>' + str(img) + '</name>\\n')\n        xml_file.write('        <pose>Unspecified</pose>\\n')\n        xml_file.write('        <truncated>1</truncated>\\n')\n        xml_file.write('        <difficult>0</difficult>\\n')\n        xml_file.write('        <bndbox>\\n')\n        xml_file.write('            <xmin>' + \"0\" + '</xmin>\\n')\n        xml_file.write('            <ymin>' + \"0\" + '</ymin>\\n')\n        xml_file.write('            <xmax>' + str(width) + '</xmax>\\n')\n        xml_file.write('            <ymax>' + str(height) + '</ymax>\\n')\n        xml_file.write('        </bndbox>\\n')\n        xml_file.write('    </object>\\n')\n        xml_file.write('</annotation>')\n\t\t\n```\n##\txml转csv文件合并csv文件\n\n要使用如下脚本将xml文件转化为csv文件、最后再把每个目录下的csv文件进行合并（注意删除重复的lable）\n\n```python\n#xml转csv文件合并csv文件\n\nimport os\nimport glob\nimport pandas as pd\nimport xml.etree.ElementTree as ET\ntag=['class1','class2']\nnum=0\n\ndef xml_to_csv(path):\n    xml_list = []\n    for xml_file in glob.glob(path + '/*.xml'):\n        tree = ET.parse(xml_file)\n        root = tree.getroot()\n        for member in root.findall('object'):\n            value = (root.find('filename').text,\n                     int(root.find('size')[0].text),\n                     int(root.find('size')[1].text),\n                     root.find('folder').text,\n                     int(member[4][0].text),\n                     int(member[4][1].text),\n                     int(member[4][2].text),\n                     int(member[4][3].text)\n                     )\n            xml_list.append(value)\n    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n    xml_df = pd.DataFrame(xml_list, columns=column_name)\n    return xml_df\n\n\ndef main():\n    for a in tag:\n        image_path = os.path.join(os.getcwd(), 'input2\\\\'+a)\n        xml_df = xml_to_csv(image_path)\n        xml_df.to_csv('data\\\\'+str(a)+'.csv',index=None)\n        print('Successfully converted xml to csv.')\n\n\nmain()\n```\n通过shell批量合并csv\n```shell\n@echo off\nE:\ncd add\ndir\ncopy *.csv all_keywords.csv\necho 合并成功！'\npause\n```\n\n## 调用object_detection前的准备\n\n下面是很有参考性的博客和官方的地址\n[https://blog.csdn.net/honk2012/article/details/79099651](https://blog.csdn.net/honk2012/article/details/79099651)\n[https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md)\n可以翻墙的话推荐下面这篇、这个towardsdatascience还是很不错的\n[https://towardsdatascience.com/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9](https://towardsdatascience.com/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9)\n基本后面的训练和模型的调用都是在github上的、想普通的个人电脑用ssd的一个mobile就行了、别的根本跑不动、batch设置的越大每次迭代的时间越长、如果太大电脑配置不够的话你就可以重新开机了、、\n顺便说说几个坑官方步骤中的 protoc object_detection/protos/*.proto --python_out=. 如果是在window下要下载3.4版本的3.5会有bug\nobject_detection初始化一定要先执行、不然会给你各种报错、、\n官方文档中export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim  如果是windows下执行要用这个命令(查了很久用了很多的坑爹方法、只能说项目对windows不友好)SET PYTHONPATH=%cd%;%cd%\\slim  执行目录还是不变\n注意这几个坑基本就会很顺畅了、还有一些其他小坑一时想不起来、想到了再加、\n\n\n\n\n\n","tags":["图像识别"],"categories":["机器学习"]},{"title":"博客搬家","url":"/2018/03/23/2018-3-23/","content":"<Excerpt in index | 首页摘要>\n无意间看到了Hexo的这个黑蓝主题、实在是太cool了！！抽空用了两个晚上搬家\n<!-- more -->\n原来的博客一直是用的是jekyll(差点又拼错、)、还是很方便不过还是有很多弊端\n\n1、代码高亮、现在看看原来的博客这代码高亮、、简直无法直视、虽然后来另外装了插件但还是惨不忍睹(主要是这个主题的高亮真的是太漂亮了、看了会上瘾、、)\n2、由于原来的博客用的是老外的主题为了实现想要的效果文字间的空格符有点受不了、十分影响美观、还有字体(这里支持一下国产、、)\n3、这个主题有分类功能、随着博客的增多查找也比原来的方便、\n4、也是主要原因、、就是想换、笑死、、、\n\n现在终于换好了、过程也十分折腾、也遇到了各种坑、什么Hexo的版本问题、server要独立安装、、、希望这博客可以用几年吧、、同时再次感谢maochunguang提供的主题\n\n前端真的是一个十分神奇的东西、、但真的没工夫投在上面学了、还有评论功能、看了大佬的主题demo觉得加了评论就不是十分洁简了、于是就不做了（绝不是因为懒）、、\n\n最后注意我的背景:它是会变的哦、、、\n","tags":["other"],"categories":["other"]},{"title":"基于卷积的图片识别","url":"/2018/03/21/2018-3-22/","content":"<Excerpt in index | 首页摘要>\n这篇博客主要介绍通过Tesorflow来实现对图片的识别\n<!-- more -->\n\n学习深度学习断断续续也将近半年了、从去年暑假接触tensorflow一步步从入门到放弃、又继续现在才算明白每一步做的是什么、本来想深入研究词向量分析做一个在线翻译的小项目和属于自己的siri（这一定非常cool）、在导师的建议下先从图像识别做起、语义模型的确太复杂了只能怪中国语言博大精深（笑死、、）可能做了一两年最终的结果将会出乎意料的喜感、不得不赞叹一下油管、、在线翻译实在是太强大了要是哪天能在谷歌工作就好了、不知不觉敲了好多废话、该写总结了\n以下是写的很详细详细的链接、看不懂的可以再细细的看这个链接看个权重的动态图就行了、绝对精髓\n[https://www.2cto.com/kf/201607/522441.html](https://www.2cto.com/kf/201607/522441.html)\n\n首先从输入的图片开始、mnist是28x28的单颜色通道的图片、训练时读取的是[batchsize,784]的数组、要转化为tensorflow卷积支持的输入格式[batchsize,28,28,1]、第二、三个表示几乘几的图片、最后一个表示颜色通道、这里为1因为是灰度图、接下来定义卷积的权重、就是你要定义一个移动的的过滤器来扫描这个图片以及若干个内核来存储扫描器与图片权值相乘再加上偏置值的一个结果、最终就可以得到卷积层的输出、需要定义的参数参考这篇博客十分的详细\n[https://www.cnblogs.com/qggg/p/6832342.html](https://www.cnblogs.com/qggg/p/6832342.html)\n\n卷积层得到输入后将其导入池化层、池化层大大减小了变量的个数（真的十分敬佩模型的创始人、真的太厉害了）、池化层也有类似的过滤器如果用的是max_pool相当于扫描一个区域、选出区域中最大的一个值输出、按照步长移动再扫描输出、从而最终达到简化参数的目的、池化层输出后将结果导入全连结层、然后就是固定的套路了、\n\n具体代码如下\n```python\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\n\nmnist=input_data.read_data_sets(\"MNIST_data\",one_hot=True)\nbatch_size=100\nn_batch=mnist.train.num_examples//batch_size\n\n\ndef weight_init(shape):\n    init=tf.truncated_normal(shape=shape,stddev=0.1)\n    return tf.Variable(init)\n\ndef bias_init(shape):\n    init=tf.constant(0.1,shape=shape)\n    return  tf.Variable(init)\n\ndef conv2d(input,w):\n    return tf.nn.conv2d(input,w,strides=[1,1,1,1],padding='SAME')\n\ndef pool(input):\n    return tf.nn.max_pool(input,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n\nx=tf.placeholder(tf.float32,[None,784])\ny=tf.placeholder(tf.float32,[None,10])\n#全0填充从一开始移动\ninput=tf.reshape(x,[-1,28,28,1])\n#定义卷积的深度为32\n#第一层卷积的输入[128,28,28,1]\nw_conv1=weight_init([5,5,1,16])\nb_conv1=weight_init([16])\n#定义的是same有0来填充每次管道的核心将会一次经过每个像素点\nconv1=tf.nn.relu(conv2d(input,w_conv1)+b_conv1)\n#第一层卷积输出[128,28,28,16]\n\n#池化层只在指定的2，3维度上进行池化\n#得到池化层的输出[128,14,14,16]\npool1=pool(conv1)\n\n#对应池化层的输出所以第三位为32此处定义深度为64\nw_conv2=weight_init([5,5,16,64])\nb_conv2=weight_init([64])\n\n#卷积的输出[128,14,14,64]\nconv2=tf.nn.relu(conv2d(pool1,w_conv2)+b_conv2)\n#得到池化的最终输出[128,7,7,64]\npool2=pool(conv2)\n\n#定义全连结层的权重\nweight=weight_init([7*7*64,500])\nbias=bias_init([500])\n\nnormal=tf.reshape(pool2,[-1,7*7*64])\n#[-1,1024]\noutput1=tf.nn.relu(tf.matmul(normal,weight)+bias)\nkeep=tf.placeholder(tf.float32)\n#定义dropout防止过拟合对提高准确率有很大的帮助\ndrop=tf.nn.dropout(output1,keep)\n\nweight2=weight_init([500,10])\nbias2=bias_init([10])\n#最终得到的输出数组的每一个权值不一定是0，1、Softmax然后会正则化这些权重值、使它们的总和等于1、以此构造一个有效的概率分布\nprediction=tf.nn.softmax(tf.matmul(drop,weight2)+bias2)\n\ncross_entropy=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n#这里用AdamOptimizer的效果要比梯度下降要好\ntrain_step=tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\ncorrect_prediction=tf.equal(tf.arg_max(prediction,1),tf.arg_max(y,1))\naccuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(100):\n        for batch in range(50):\n            batch_xs,batch_ys =mnist.train.next_batch(batch_size)\n            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep:0.7})\n        acc=sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep:1.0})\n        print('iter'+str(epoch)+\"  correct \"+str(acc))\n    input_image = mnist.train.images[11:12]\n    # 可视化卷积层学习到的特征\n    # 输入一张图片\n    cnn1=sess.run(w_conv1, feed_dict={x:input_image})\n    conv1_reshape = sess.run(tf.reshape(cnn1, [5, 5, 1, 16]))\n    plt.figure()\n    # 放在两行两列第一个位置#将舍去数组的后两位\n    plt.subplot(2, 2, 1)\n    # 将舍去数组的后两位\n    plt.imshow(conv1_reshape[:,:,0,0])\n    plt.title('Conv1 16x28x28')\n    plt.show()\n\n```\n## 参数的计算\n假设N*N为输入图像的size、F*F是filter(卷积核)的size、stride(即卷积核每次移动的像素)是滑动的步长。\n那么一次卷积之后输出的第一个维度为(N-F)/stride +1\n\n下面是一篇关于交叉熵的问题的博客\n[http://blog.csdn.net/john_xyz/article/details/61211422](http://blog.csdn.net/john_xyz/article/details/61211422)\n笔记本配置比较一般、渴望gpu来拯救、由于训练的比较慢又要不断调整参数最终准确率在97%以上是没问题的\n## 图像识别\n一般做图像识别用到的模型在github上都开源出来了、比如inception3就有基于Tensorflow的了、用inception3训练自己模型时卷积层的参数大致是不变的改变的是顶部神经元的参数\n前面的操作差不多做的是特征提取、所以用自己的数据训练后得到的结果还是不错的、\n下面是谷歌物体识别的连接、下面的模型可以拿来直接用不用自己一层一层搭网络、里面也有已经训练好的模型(当自己想要做点什么的时候谷歌都做好了、、、)\n[https://github.com/tensorflow/models/tree/master/research/object_detection](https://github.com/tensorflow/models/tree/master/research/object_detection)\n环境搭建推荐linux、windows的坑太多浪费了好长时间、官方给的教程十分精辟、要注意每次敲命令行要严格！！！对应注释中给的目录\n最后还可以训练自己的数据集、教程官方github上也有、网上的教程也十分多参照一下就好了（不想再做验证性工作了）、、\n","tags":["cnn"],"categories":["机器学习"]},{"title":"K-近邻算法Python实现","url":"/2018/01/30/2018-1-30/","content":"<Excerpt in index | 首页摘要>\n运用python通过计算距离来实现对某花的分类\n<!-- more -->\n## 算法解决的问题\n已知样本集（此处的样本为某花的实例数据）、给定一未知样本的数据来断此样本的类别(此处为判断属于哪一类花）\n## 解决步骤\n特征抽取后计算出未知样本到所有已知样本的距离、根据给定参数K（最好为奇数便于投票）选出K个最近的样本点、统计出类别最多的样本点的类别、最终的的分类就是该类别\n缺陷：数据的分布不均匀会导致结果的不准确\n优化方法：根据距离的远近添加相应的权重来弱化数据分布不均匀的为题（下面代码还没实现权重的添加、、以后有空再加、、、）\n个人脑洞：对于多维的数据、在二维分布上可能看不出任何规律、但在高维的空间中明显的可以分开好几个类别（如本例的某花数据在三维下就很明显了、还有支持向量机的划分方法太cool了）\n此处的样本集（非常nice的数据集大全）\n[http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data](http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data)\n样本集示例：前四列为花的数据、最后为花的类别\n\n5.1,3.5,1.4,0.2,Iris-setosa\n\n5.0,3.3,1.4,0.2,Iris-setosa\n\n7.0,3.2,4.7,1.4,Iris-versicolor\n\n4.6,3.1,1.5,0.2,Iris-setosa\n\n6.4,3.2,5.3,2.3,Iris-virginica\n\n6.9,3.2,5.7,2.3,Iris-virginica\n\n4.6,3.4,1.4,0.3,Iris-setosa\n\n代码实现如下：用测试集测试可以达到96%的准确率\n\n```python\n\nimport csv\nimport math\nfrom collections import Counter\n\n#导入样本集list\n#导入测试集计算测试集到每个样本集的距离,结果保存为list\n#根据distance排名取k个投票选出最多的这个类\n\n\n#传递时要第二个参数要为空参否则会共用同一个地址\ndef readfile(local):\n    dataset=[]\n    with open(local) as file2:\n        csv_reader = csv.reader(file2)\n        for line in csv_reader:\n            dataset.append(line)\n    return dataset\n\n\ndef distance (test,train):\n    result=0.0\n    #此时每个test例如[1,2,3,4]每个train例如[1,2,3,4,a],-1除去标签\n    for i in  range(len(test)-1):\n        result=result+math.sqrt(abs((float(test[i])-float(train[i]))*(float(test[i])+float(train[i]))))\n    return result\n\ndef sort(train,test,k=3):\n    result=[]\n    sortresult=[]\n    #计算每个样本集到样本的距离\n    for i in range(len(test)):\n        for m in range(len(train)):\n            #对于每个测试实例得到距离和对应的标签\n            result.append([distance(test[i],train[m]),train[m][-1]])\n        sortresult.append(findsort(result,k))#得到每一个测试集的分类结果\n        result=[]                            #将每个测试集的距离集合清空\n    return sortresult       #最终结果\n    #得到结果集，每一个test到样本集的距离\n\n#示例输入[[3.917258917468777, 'Iris-setosa'], [4.365595716195167, 'Iris-setosa']]\ndef findsort(data,k=3):\n    result={}\n    voat=[]\n    for x in range(len(data)):\n            result.update({data[x][0]:data[x][1]})\n    #对字典进行排序从小到大\n    a=sorted(result.items(), key=lambda d: d[0])\n    for m in range(k):\n         voat.append(a[m][-1])\n    #得到列表中出现次数最多的元素\n    b=Counter(voat).most_common(1)\n    return b[0][0]\n\n#计算准确率\ndef correct(sample,predict):\n    flag=0\n    for a in range(len(sample)):\n        if(sample[a]==predict[a]):\n            flag=flag+1\n    return flag/len(sample)\n\ndef main():\n    testlist=[]\n    train=list(readfile(\"F:\\\\train.csv\"))\n    test=list(readfile(\"F:\\\\test.csv\"))\n    #k为最近邻的个数\n    output=sort(train,test,3)\n    #得到分类的结果集\n    print(output)\n    for a in range(len(test)):\n        testlist.append(test[a][-1])\n    #输出准确率\n    print(correct(testlist,output))\nmain()\n\n```\n\n\n\n\n\n\n\n\n\n\n","tags":["python"],"categories":["机器学习"]},{"title":"基本神经网络","url":"/2017/12/10/2017-12-10/","content":"<Excerpt in index | 首页摘要>\n简单整理一下神经网络训练的步骤\n<!-- more -->\n总结一下最简单的神经网络的训练过程和原理\n通常利用数据交叉验证来提高数据利用率\n<img src=\"http://aRootUser.github.io/img/2/1.jpg\">\n交叉验证：给定一个训练集和测试集，为了最大程度的利用测试集，可以将训练集分为若干份，这里为5。第一次将fold1(折)作为测试集其余的作为训练集，第二次将fold2作为测试集，其余的作为训练集，以此类推从而达到最大化利用数据更新权重的效果\n<img src=\"http://aRootUser.github.io/img/2/2.jpg\">\n对于输入的一张图片简单将图片的输入像素点看成[1,4]的矩阵、输出层为[1,3],中间的权值为[4,3]的矩阵、和图中不同图中是左成矩阵、这里定义的是右乘矩阵、没有定义中间层、最后还要加上[1,3]偏置值得到[1,3]的输出值每一个值代表某一类别的得分、\n<img src=\"http://aRootUser.github.io/img/2/3.jpg\">\n为了更好的定以中间权值定义的好坏以及预测结果的准确程度、用损失函数来衡量、损失函数最小表示预测越准确、这里定义的是svm损失函数、\nl 表示自己定义的可容忍的长度\nyi表示正确类别的得分\nj表示其他类别的得分\n通过计算每个其他类别减去正确类别的得分的最大值的求和来表是损失函数的结果对于多个输入例如输入100张图片还要除去100相当于取平均值\n<img src=\"http://aRootUser.github.io/img/2/4.jpg\">\n为了防止权值为0从而导致输入样本的每一个值没有被充分利用例如训练得到的两个权值\n设输入的样本为[1,1,1,1]\n权重W1[0.5,0.5,0.5,0.5]    \n权重W2[1,0,0,0]\n矩阵相乘后得到的结果相同但是w2由于有三个0没有充分利用每一项所以添加w的平方项来惩罚权重为w2的情况、使其损失值变大\t\n<img src=\"http://aRootUser.github.io/img/2/5.jpg\">\n<img src=\"http://aRootUser.github.io/img/2/6.jpg\">\n分类器的作用将输出的值通过sigmoid函数映射到0至1的区间上、e的x次幂进行放大、最后通过取其作为正确类别的概率取负对数得最终其对应的损失值(因为概率越大越输出的损失值越小)\n前向传播：从输入的x一直到计算出loss、通过梯度下降算法找到一个下降方向、最终找到最低点、训练的批次数一般为2的整数次幂\n一个Epoch表示迭代完所有数据、一个迭代表示跑完当前的一个batch\n## 学习率\n每次训练跟新权重的变化要乘一个学习率来调整权值变化的大小、过大会错过最优解\n## 反向传播\n通过计算出每一个权重对最终的loss值的影响来调整权重的大小(向前传播的逆向求解)\n## 激活函数 \n对神经元的输出进行去线性化、例如sigmoid函数(由于当x过大时很容易导致梯度消失使其无法求导进行反向传播、现在一般用relu激活函数并且求导简单）\n## 过拟合问题 \ndrop-out进行处理通过迭代来弥补神经网络的复杂度\n## 过程小结 \n首先输入训练集如手写数字集、定义神经网络后、通过向前传播得到对每一个类别的输出、通过sortmax函数将输出转化为概率分布、通过与标签进行如下运算个（标签是one-hot概率）、将输出的概率分布取对数与标签值乘积在做平均值求和最后取负数-tf.reduce_sum(y_*tf.log(y))、得到交叉熵来反应结果集与标签的相似度、最后通过梯度下降法不断训练使交叉熵最小、来优化权重参数、\n\n\n\n\n\n\n\n\n\n\n","tags":["机器学习"],"categories":["机器学习"]}]